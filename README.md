# Processo de ETL @ Ra√≠zen Analytics


Este documento apresenta o projeto t√©cnico de um Pipeline ETL de Vendas de Combust√≠veis da ANP (Ag√™ncia Nacional do Petr√≥leo, G√°s Nautral e Biocombust√≠veis), detalhando o processo de extra√ß√£o e estrutura√ß√£o dos dados de tabelas din√¢micas.

O objetivo desse projeto √© criar um processo eficiente de ETL, de modo a coletar e organizar dados de vendas de combust√≠veis de forma consistente e confi√°vel. Para isso, foram utilizadas ferramentas de ETL como o Apache Airflow.

O Pipeline ETL foi desenvolvido em fases distintas, incluindo extra√ß√£o dos dados brutos, transforma√ß√£o dos dados para adequ√°-los √†s necessidades de an√°lise, e carregamento dos dados em um banco de dados.

Tabelas de Refer√™ncia:

 * Vendas, pelas distribuidoras, dos derivados combust√≠veis de petr√≥leo por Unidade da Federa√ß√£o e produto - 2000-2023 (m¬≥)

![tab_din_petroleo](https://user-images.githubusercontent.com/116772002/228603848-6e4a670e-1903-4a83-97f0-3e8275d95588.jpg)


 * Vendas, pelas distribuidoras, de √≥leo diesel por tipo e Unidade da Federa√ß√£o - 2013-2023 (m¬≥)

![tab_din_diesel](https://user-images.githubusercontent.com/116772002/228603894-d7aff146-8221-439d-bf01-2b83cfe8351c.jpg)




# ‚öôÔ∏è Extra√ß√£o dos Dados

Arquivo com dados brutos disponibilizados pela [ANP](https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-estatisticos) (Ag√™ncia Nacional do Petr√≥leo, G√°s Nautral e Biocombust√≠veis).
 



# üî© Dicion√°rio de Dados


* <b>Tabelas Stage (√ìleo Diesel e Derivados Combust√≠veis de Petr√≥leo)</b>

| **vari√°vel** |   **tipo**  |            **descri√ß√£o**           |
|:------------:|:-----------:|:----------------------------------:|
|  COMBUST√çVEL | VARCHAR(30) | Derivados Combust√≠veis de Petr√≥leo ou √ìleo Diesel|
|      ANO     |     INT     |        Ano referente a venda       |
|    REGI√ÉO    | VARCHAR(50) |          Regi√£o do Brasil          |
|    ESTADO    | VARCHAR(50) |          Estado do Brasil          |
|    UNIDADE   |  VARCHAR(3) |           metros c√∫bicos           |
|      Jan     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Fev     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Mar     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Abr     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Mai     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Jun     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Jul     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Ago     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Set     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Out     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Nov     |    FLOAT    |      Vendas referentes ao m√™s      |
|      Dez     |    FLOAT    |      Vendas referentes ao m√™s      |
|     TOTAL    |    FLOAT    |      Vendas referentes ao m√™s      |


* <b>Tabelas Final (√ìleo Diesel e Derivados Combust√≠veis de Petr√≥leo)</b>

| **vari√°vel** |   **tipo**  |                   **descri√ß√£o**                   |
|:------------:|:-----------:|:-------------------------------------------------:|
|  year_month  |     DATE    |            Data de refer√™ncia da Venda            |
|      uf      | VARCHAR(50) |                  Estado do Brasil                 |
|    product   | VARCHAR(30) | Derivados Combust√≠veis de Petr√≥leo ou √ìleo Diesel |
|     unit     |  VARCHAR(2) |                   metros c√∫bicos                  |
|    volume    |    FLOAT    |                       Vendas                      |
|  created_at  |   DATETIME  |                  Data do registro                 |
|              |             |                                                   |



# üì¶ Desenvolvimento





Para o desenvolvimento deste projeto, foram realizados dois processos de ETL, um sendo realizado de forma local com Python e o outro processo utilizando o Apache Airflow, sendo processado em um container Docker. 

Nesta se√ß√£o de desenvolvimento, ser√° detalhado cada um dos processos de ETL.

Verificando as tabelas din√¢micas do arquivo xlsx disponibilizado pela ANP (Ag√™ncia Nacional do Petr√≥leo, G√°s Natural e Biocombust√≠veis), foi utilizada a op√ß√£o de visualizar os dados brutos referentes √† tabela din√¢mica para ter acesso a todas as informa√ß√µes de vendas disponibilizadas pela ANP.



## ETL - Python

- Arquitetura do Processo de ETL com Python
![arquitetura_v1](https://user-images.githubusercontent.com/116772002/228604438-5e470910-6453-4e9f-b2af-4223bbe9beba.png)

Ap√≥s a verifica√ß√£o do arquivo com os dados brutos, foi realizada a leitura desses dados utilizando Python, onde foram lidos os dados brutos referentes √†s informa√ß√µes de vendas de Derivados Combust√≠veis de Petr√≥leo e √ìleo Diesel. Realizou-se uma an√°lise explorat√≥ria dos arquivos, criou-se um dataframe e concatenou-se as informa√ß√µes relevantes sobre as vendas. 

O dataframe resultante cont√©m 4968 linhas e 18 colunas para as informa√ß√µes sobre Petr√≥leo e um dataframe com 1350 linhas e 17 colunas para as informa√ß√µes sobre √ìleo Diesel. Os dataframes foram enviados para o schema Stage no banco de dados SQL Server, utilizando uma fun√ß√£o para conectar com o banco de dados por meio da biblioteca SQLAlchemy.

Shape e Tabela Stage (petr√≥leo)

![df_stage_pet_shape](https://user-images.githubusercontent.com/116772002/228604895-d7df8cdc-fbec-45aa-b860-1b15a1e00ea9.jpg)

![stage_petroleo_sqlserver](https://user-images.githubusercontent.com/116772002/228605474-2206b514-5124-4038-99cb-7798c6059d29.jpg)

Shape e Tabela Stage (diesel)

![df_stage_diesel_shape](https://user-images.githubusercontent.com/116772002/228604935-5cee69c0-db28-44a1-bed6-dc03a94dcd8d.jpg)

![stage_diesel_sqlserver](https://user-images.githubusercontent.com/116772002/228605510-2110e1f2-3c0b-4983-ab93-7d2a6e69f586.jpg)




Em outro script Python, foi realizada a leitura desses dados brutos no schema Stage e armazenados em um dataframe stage. Em seguida, foram criadas algumas vari√°veis auxiliares para a organiza√ß√£o dos dados do dataframe final. Ent√£o, foi criada uma fun√ß√£o para concatenar as informa√ß√µes e realizar a cria√ß√£o desse dataframe final.
Realizando itera√ß√µes utilizando o m√©todo iterrows, foi realizado o mapeamento dos valores referentes ao produto, ano e m√™s para inserir os dados no novo dataframe. 
Chegando no dataframe final referente √†s informa√ß√µes sobre Petr√≥leo com 59616 linhas e 6 colunas e um dataframe final com as informa√ß√µes sobre √ìleo Diesel  16200 linhas e 6 colunas.

Shape e Tabela Final (petr√≥leo)

![df_final_pet_shape](https://user-images.githubusercontent.com/116772002/228605597-fa7a3b31-8de5-4d63-b5d4-ef825ca06f2e.jpg)
 
![final_petroleo_sqlserver](https://user-images.githubusercontent.com/116772002/228605703-d9f0b6c3-dfae-4994-8c09-570b53f8243b.jpg)

Shape e Tabela Final (diesel)

![df_final_diesel_shape](https://user-images.githubusercontent.com/116772002/228605618-aaf011ed-b7dd-45e7-bcb7-b3a2a1a2fe08.jpg)

![final_diesel_sqlserver](https://user-images.githubusercontent.com/116772002/228605729-31df4ac6-98a6-41e8-8f53-63c948f99b81.jpg)




Os dataframes finais foram enviados para o schema Final no banco de dados SQL Server utilizando uma fun√ß√£o para conectar ao banco de dados.

Ap√≥s a inser√ß√£o dos dados no banco de dados, foram realizadas consultas de valida√ß√£o para obter o somat√≥rio dos valores correspondentes √† tabela din√¢mica. Essas consultas foram executadas com o objetivo de verificar se os dados inseridos foram armazenados corretamente e se os c√°lculos est√£o sendo feitos de forma precisa. Essa etapa √© fundamental para garantir a integridade dos dados e evitar erros nos resultados obtidos.

## ETL - Airflow 

 - Arquitetura utilizando Apache Airflow
![Diagrama sem nome drawio (2)](https://user-images.githubusercontent.com/116772002/228604080-2fa7076b-1c60-4bd6-b869-4b5c545d8d81.png)

Para implantar o Airflow em um container Docker, utilizou-se um arquivo YAML fornecido pela Apache Airflow que cont√©m v√°rios servi√ßos essenciais para o funcionamento da ferramenta, tais como:
 - airflow-scheduler - respons√°vel por monitorar todas as tarefas e DAGs e acionar as inst√¢ncias de tarefa assim que suas depend√™ncias s√£o conclu√≠das.

 - airflow-webserver - o servidor web que permite acessar a interface gr√°fica do Airflow.

 - airflow-worker - worker que executa as tarefas definidas pelo escalonador.

 - airflow-triggerer - acionador executa um loop de eventos para tarefas adi√°veis.

 - airflow-init - servi√ßo de inicializa√ß√£o.

 - postgres - banco de dados utilizado pelo Airflow.

 - redis - broker que encaminha mensagens do escalonador para o worker.

![docker](https://user-images.githubusercontent.com/116772002/228605785-191f3515-978f-4990-a1ce-bd23906ef955.jpg)



Referente ao processo de ETL sendo orquestrado pelo Apache Airflow, o desenvolvimento do c√≥digo foi semelhante, sendo necess√°rio alterar algumas fun√ß√µes para adequar √†s necessidades de processamento de dados no Airflow e tamb√©m ao envio das informa√ß√µes para um banco de dados. Neste caso, foi utilizado o banco de dados PostgreSQL para armazenar os dados do dataframe stage e final.

![pipeline_air](https://user-images.githubusercontent.com/116772002/228634072-c1f45478-092f-4771-878c-171e699c9fe7.jpg)

Amostra de Dados inseridos no PostgreSQL

![stage_postgres](https://user-images.githubusercontent.com/116772002/228637651-0c57dd80-cfc7-4296-a4a6-3b275b54489c.jpg)









# üõ†Ô∏è Constru√≠do com


* [Python](https://www.python.org/)
* [Microsoft SQL Server Management Studio](https://www.microsoft.com/pt-br/sql-server/sql-server-downloads) - Banco de dados SQL Server
* [Apache Airflow](https://airflow.apache.org/) - Orquestrador de Fluxo de Trabalho (Pipelines)
* [Docker](https://www.docker.com/products/docker-desktop/) - Ferramenta para cria√ß√£o, execu√ß√£o e gerenciamento de cont√™ineres 
* [PostgreSQL](https://www.postgresql.org/) - Banco de dados PosgreSQL


# üèÉ Autor


* **Fernando Barbosa** - *Engenheiro de Dados Jr* - [github](https://github.com/FernandoBarbosaB)
